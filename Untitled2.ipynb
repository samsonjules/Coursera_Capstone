{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import bs4\n",
    "import random\n",
    "import requests\n",
    "# !pip install fake-useragent\n",
    "from fake_useragent import UserAgent\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'https://www.jobpaw.com/pont/professionnels.php?idj=12100'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(token, nb):\n",
    "    pages = []\n",
    "    for i in range(1,nb+1):\n",
    "        j = token + str(i)\n",
    "        pages.append(j)\n",
    "    return pages\n",
    "\n",
    "pages = get_pages(token,14295)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# https://www.proxy-list.download/HTTPS\n",
    "proxies = pd.read_csv('proxy_list.txt', header = None)\n",
    "proxies = proxies.values.tolist()\n",
    "proxies = list(it.chain.from_iterable(proxies))\n",
    "\n",
    "def get_data(pages,proxies):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    parameters = ['Titre du poste','Domaine','Date publication','Pays','Zone','Compagnie','Spécialité','Date limite','Ville','Durée']\n",
    "    ua = UserAgent()\n",
    "    proxy_pool = it.cycle(proxies)\n",
    "    \n",
    "    while len(pages) > 0:\n",
    "        for i in pages:\n",
    "        # on lit les pages une par une et on initialise une table vide pour ranger les données d'une page     \n",
    "            df_f = pd.DataFrame()\n",
    "        # itération dans un liste de proxies    \n",
    "            proxy = next(proxy_pool)\n",
    "        # essai d'ouverture d'une page   \n",
    "            try:\n",
    "                response = requests.get(i,proxies={\"http\": proxy, \"https\": proxy}, headers={'User-Agent': ua.random},timeout=5)\n",
    "                time.sleep(random.randrange(1,5))\n",
    "        # lecture du code html et la recherche des balises <em>\n",
    "                soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "                table_box = soup.find_all(\"tbody\")#, {\"class\":\"table table-bordered hidden-xs\"})\n",
    "        # extraction des données        \n",
    "                for par in parameters:\n",
    "                    l = []\n",
    "                    for el in em_box:\n",
    "                        j = el[par]\n",
    "                        l.append(j)\n",
    "                    l = pd.DataFrame(l, columns = [par])\n",
    "                    df_f = pd.concat([df_f,l], axis = 1)\n",
    "                df = df.append(df_f, ignore_index=True)\n",
    "                pages.remove(i)\n",
    "                print(df.shape)\n",
    "            except:\n",
    "                print(\"Skipping. Connnection error\")\n",
    "                \n",
    "    return df\n",
    "\n",
    "data = get_data(pages,proxies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token= 'https://www.jobpaw.com/pont/professionnels.php?idj='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "&totalRows_RS_job=10642&id=55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(token, nb):\n",
    "    pages = []\n",
    "    for i in range(1,nb+1):\n",
    "        j = token + str(i)\n",
    "        pages.append(j)\n",
    "    return pages\n",
    "\n",
    "pages = get_pages(token,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# https://www.proxy-list.download/HTTPS\n",
    "#proxies = pd.read_csv('proxy_list.txt', header = None)\n",
    "#proxies = proxies.values.tolist()\n",
    "#proxies = list(it.chain.from_iterable(proxies))\n",
    "\n",
    "def get_data(pages):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    parameters = ['Titre du poste','Domaine','Date publication','Pays','Zone','Compagnie','Spécialité','Date limite','Ville','Durée']\n",
    "    ua = UserAgent()\n",
    "    #proxy_pool = it.cycle(proxies)\n",
    "    \n",
    "    while len(pages) > 0:\n",
    "        for i in pages:\n",
    "        # on lit les pages une par une et on initialise une table vide pour ranger les données d'une page     \n",
    "            df_f = pd.DataFrame()\n",
    "        # itération dans un liste de proxies    \n",
    "            #proxy = next(proxy_pool)\n",
    "        # essai d'ouverture d'une page   \n",
    "            try:\n",
    "                response = requests.get(i)#, headers={'User-Agent': ua.random},timeout=5)\n",
    "                time.sleep(random.randrange(1,5))\n",
    "        # lecture du code html et la recherche des balises <em>\n",
    "                soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "                em_box = soup.find_all(\"div\", {\"class\":\"content-page\"})\n",
    "        # extraction des données        \n",
    "                for par in parameters:\n",
    "                    l = []\n",
    "                    for el in em_box:\n",
    "                        j = el[par]\n",
    "                        l.append(j)\n",
    "                    l = pd.DataFrame(l, columns = [par])\n",
    "                    df_f = pd.concat([df_f,l], axis = 1)\n",
    "                df = df.append(df_f, ignore_index=True)\n",
    "                pages.remove(i)\n",
    "                print(df.shape)\n",
    "                print(df)\n",
    "            except:\n",
    "                print(\"Skipping. Connnection error\")\n",
    "                \n",
    "    return df\n",
    "\n",
    "data = get_data(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_f, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
